# Research: Malware URL Detection API (Phase 0)

**Date**: 2025-12-30  
**Feature**: 001-malware-url-detection  
**Purpose**: Resolve technical unknowns and validate design decisions before implementation

## Research Questions Addressed

### 1. Multi-Database Loader Pattern

**Question**: How should we structure support for multiple malware databases (files, HTTP endpoints, future SQL)?

**Decision**: Pluggable Loader Pattern (Strategy Pattern)

**Rationale**:
- Clean abstraction with abstract `BaseLoader` interface
- Each loader type (FileLoader, HTTPLoader, SQLLoader) inherits and implements its own async `lookup(url)` method
- Malware checker orchestrates across multiple loaders in parallel
- Low coupling: adding new database source requires new loader only, no changes to core logic
- Testable: each loader can be tested and mocked independently

**Implementation Approach**:
```python
# services/database_loaders/base.py
class BaseLoader(ABC):
    @abstractmethod
    async def lookup(self, url: str) -> Optional[ThreatInfo]:
        """Query database for URL, return threat info if found."""
        pass

# services/database_loaders/file_loader.py
class FileLoader(BaseLoader):
    async def lookup(self, url: str) -> Optional[ThreatInfo]:
        # Load from CSV/JSON file asynchronously
        pass

# services/database_loaders/http_loader.py  
class HTTPLoader(BaseLoader):
    async def lookup(self, url: str) -> Optional[ThreatInfo]:
        # Query external HTTP endpoint asynchronously
        pass

# services/malware_checker.py
async def check_url(url: str) -> bool:
    results = await asyncio.gather(
        file_loader.lookup(url),
        http_loader.lookup(url),
        # Add more loaders as needed
    )
    return any(results)  # Malicious if ANY loader marks it
```

**Alternatives Considered**:
- Single hardcoded database: Would require code changes for each new data source
- Configuration-driven factory: Adds complexity; loader pattern more explicit and maintainable

**Validation**: Pattern supports current requirements and enables future sources without code restructuring.

---

### 2. Async/Await Strategy for Concurrent Requests

**Question**: How to maximize concurrency while keeping code simple and avoiding bottlenecks?

**Decision**: FastAPI + asyncio with connection pooling

**Rationale**:
- FastAPI's async request handlers automatically manage concurrent request queuing
- `asyncio.gather()` for parallel database queries within a single request
- httpx async client with connection pooling prevents connection exhaustion
- aiofiles for async file I/O in file loader (no blocking)
- Timeouts on all external calls prevent indefinite hangs
- Proper task cancellation handling in error cases

**Implementation Approach**:
```python
# main.py
app = FastAPI()

# Shared async clients (connection pooling)
http_client = httpx.AsyncClient(limits=httpx.Limits(max_connections=100))

@app.get("/urlinfo/1/{hostname_and_port}/{path:path}")
async def check_url(hostname_and_port: str, path: str = ""):
    """Handle request asynchronously; all I/O is non-blocking."""
    # Reconstruct full URL
    # Validate format
    # Gather results from all loaders in parallel
    # Return response
```

**Alternatives Considered**:
- Threading: GIL contention, harder to reason about state
- Multiprocessing: Overkill for I/O-bound workload; harder inter-process communication
- Synchronous calls with thread pool: Degrades under high concurrency vs async

**Validation**: FastAPI with asyncio is proven pattern for high-concurrency APIs; Uvicorn server manages request queueing.

---

### 3. URL Validation and Normalization Strategy

**Question**: How to handle URL format variations and validation edge cases?

**Decision**: urllib + custom normalization logic

**Rationale**:
- `urllib.parse` provides robust URL parsing (handles most edge cases)
- Custom normalizer for consistency: scheme defaults (http→https), trailing slash removal, query param sorting
- Case-insensitive domain comparison
- Length validation (reject >2048 chars)
- Early validation before any database query (fail fast)
- Clear error messages for each validation failure type

**Implementation Approach**:
```python
# services/url_validator.py
from urllib.parse import urlparse, urlunparse
from pydantic import BaseModel, validator

class URLValidator:
    @staticmethod
    def validate_and_normalize(url: str) -> str:
        """Validate URL format and return normalized version."""
        # Check length
        if len(url) > 2048:
            raise ValueError("URL exceeds maximum length")
        
        # Parse with urllib
        try:
            parsed = urlparse(url)
        except Exception:
            raise ValueError("Invalid URL format")
        
        # Require scheme and netloc
        if not parsed.scheme or not parsed.netloc:
            raise ValueError("URL must include scheme and domain")
        
        # Normalize: scheme lowercase, host lowercase, remove trailing slashes
        normalized = ...
        return normalized
```

**Edge Cases Handled**:
- Empty/null: Caught before parse, clear error
- No scheme: Error message directs to add http/https
- Query parameters and fragments: Preserved in lookup (database determines relevance)
- Non-ASCII domains: urllib handles IDN (internationalized domain names)
- http vs https: Normalized to same domain for lookup
- Connection pool limit exceeded: Timeout error with retry guidance

**Alternatives Considered**:
- Regex validation: Fragile; urllib more maintainable and robust
- No normalization: Risk of false negatives (same site marked safe and malicious)

**Validation**: urllib is standard library, well-tested; additional logic focused on consistency.

---

### 4. Caching Strategy for Performance

**Question**: How to improve response times for repeated URL lookups?

**Decision**: Optional in-memory cache with TTL and size limits

**Rationale**:
- Malware lists update periodically (not real-time), so caching safe
- In-memory cache via `cachetools` library with TTL (time-to-live) = 1 hour
- Cache size limit (e.g., 10,000 entries) prevents unbounded memory growth
- Cache key = normalized URL (ensures consistency with validation)
- Cache stores: (is_malicious, timestamp)
- Transparent: cache miss still queries databases
- Can be disabled via config for security-sensitive deployments

**Implementation Approach**:
```python
# utils/cache.py
from cachetools import TTLCache

class URLCache:
    def __init__(self, maxsize=10000, ttl=3600):
        self.cache = TTLCache(maxsize=maxsize, ttl=ttl)
    
    async def get(self, url: str) -> Optional[bool]:
        return self.cache.get(url)
    
    async def set(self, url: str, is_malicious: bool):
        self.cache[url] = is_malicious

# services/malware_checker.py
async def check_url(url: str, use_cache=True) -> bool:
    if use_cache:
        cached = await cache.get(url)
        if cached is not None:
            return cached
    
    # Query databases
    result = await query_loaders(url)
    
    if use_cache:
        await cache.set(url, result)
    
    return result
```

**Alternatives Considered**:
- Redis cache: Adds operational complexity; in-memory sufficient for MVP
- Database-level caching: Offloads to database layer; less control over TTL
- No caching: Meets baseline 500ms; caching provides buffer for scaling

**Validation**: TTL-based caching balances freshness and performance; config controls trade-offs.

---

### 5. Error Handling and Graceful Degradation

**Question**: How to handle database unavailability, timeouts, and failures transparently?

**Decision**: Structured error handling with fallback responses and logging

**Rationale**:
- Timeout on all database queries (e.g., 5 seconds per loader)
- If timeout or connection error: log error, continue checking other loaders
- If ALL loaders fail: return 503 with clear error message
- If SOME loaders fail: continue and report as "unknown" vs "safe" (conservative approach)
- Structured logging with request ID for audit trail
- Error messages distinguish between client errors (400) and server errors (500/503)

**Implementation Approach**:
```python
# services/malware_checker.py
async def check_url(url: str) -> URLCheckResponse:
    loader_results = {}
    
    for loader in self.loaders:
        try:
            result = await asyncio.wait_for(
                loader.lookup(url),
                timeout=5.0
            )
            loader_results[loader.name] = result
        except asyncio.TimeoutError:
            logger.warning(f"Loader {loader.name} timeout", extra=request_context)
            loader_results[loader.name] = None
        except Exception as e:
            logger.error(f"Loader {loader.name} failed: {e}", extra=request_context)
            loader_results[loader.name] = None
    
    # Decision logic: if ANY loader marks as malicious, return true
    is_malicious = any(loader_results.values())
    
    # If no loaders succeeded, return 503
    if all(v is None for v in loader_results.values()):
        raise HTTPException(status_code=503, detail="Database unavailable")
    
    return URLCheckResponse(
        url=url,
        is_malicious=is_malicious,
        timestamp=datetime.utcnow()
    )
```

**Alternatives Considered**:
- Fail on first error: Less resilient; one database issue blocks service
- Cache fallback on error: Hides failures; less transparent
- Circuit breaker pattern: Over-engineered for MVP; can add later if needed

**Validation**: Approach provides transparency (logging), resilience (partial failures OK), and clear error codes.

---

## Summary of Design Decisions

| Area | Decision | Benefit |
|------|----------|---------|
| Multi-DB Support | Pluggable Loader Pattern | Extensible, testable, low coupling |
| Concurrency | FastAPI + asyncio + connection pooling | High throughput, simple code, non-blocking |
| URL Handling | urllib + custom normalization | Robust, consistent, handles edge cases |
| Performance | Optional in-memory cache with TTL | Fast repeated lookups, configurable freshness |
| Error Handling | Timeout-based, partial failure OK, structured logging | Transparent, resilient, auditable |

All decisions align with the constitution's five principles:
- ✅ **Comprehensive Testing**: Edge cases in URL validation, error handling tested
- ✅ **Transparency**: Structured logging throughout async operations
- ✅ **Incremental**: Each loader independent; failures don't cascade
- ✅ **Scalability & Reliability**: Async-first, graceful degradation, connection pooling
- ✅ **Async Design**: All I/O non-blocking, proper error handling in async contexts

## Next Steps

1. Generate data-model.md with formal entity definitions
2. Generate contracts/urlinfo.md with request/response examples and error codes
3. Update agent context with FastAPI, pytest, and architecture decisions
4. Generate tasks.md with test-first task breakdown by user story
